{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 15:16:49.324124: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-27 15:16:49.430144: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-27 15:16:50.205678: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-27 15:16:53.050309: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close  \\\n",
       "0  2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1  2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2  2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3  2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4  2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "\n",
       "       Volume  \n",
       "0  30182600.0  \n",
       "1  30552600.0  \n",
       "2  36141000.0  \n",
       "3  28069600.0  \n",
       "4  29091300.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_train_csv = pd.read_csv('bases/petr4_treinamento.csv')\n",
    "base_train_csv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>16.190001</td>\n",
       "      <td>16.549999</td>\n",
       "      <td>16.190001</td>\n",
       "      <td>16.549999</td>\n",
       "      <td>16.516966</td>\n",
       "      <td>33461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>16.490000</td>\n",
       "      <td>16.719999</td>\n",
       "      <td>16.370001</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>16.666668</td>\n",
       "      <td>55940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>16.780001</td>\n",
       "      <td>16.959999</td>\n",
       "      <td>16.620001</td>\n",
       "      <td>16.730000</td>\n",
       "      <td>16.696608</td>\n",
       "      <td>37064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>16.570000</td>\n",
       "      <td>16.830000</td>\n",
       "      <td>16.796408</td>\n",
       "      <td>26958200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>16.740000</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.709999</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.996010</td>\n",
       "      <td>28400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close    Volume\n",
       "0  2018-01-02  16.190001  16.549999  16.190001  16.549999  16.516966  33461800\n",
       "1  2018-01-03  16.490000  16.719999  16.370001  16.700001  16.666668  55940900\n",
       "2  2018-01-04  16.780001  16.959999  16.620001  16.730000  16.696608  37064900\n",
       "3  2018-01-05  16.700001  16.860001  16.570000  16.830000  16.796408  26958200\n",
       "4  2018-01-08  16.740000  17.030001  16.709999  17.030001  16.996010  28400000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_test_csv = pd.read_csv('bases/petr4_teste.csv')\n",
    "base_test_csv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_csv = base_train_csv.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9990000e+01, 2.0209999e+01, 1.9690001e+01, 1.9690001e+01,\n",
       "        1.8086271e+01, 3.0182600e+07],\n",
       "       [1.9809999e+01, 2.0400000e+01, 1.9700001e+01, 2.0400000e+01,\n",
       "        1.8738441e+01, 3.0552600e+07],\n",
       "       [2.0330000e+01, 2.0620001e+01, 2.0170000e+01, 2.0430000e+01,\n",
       "        1.8766001e+01, 3.6141000e+07],\n",
       "       ...,\n",
       "       [1.5990000e+01, 1.6139999e+01, 1.5980000e+01, 1.6049999e+01,\n",
       "        1.6017963e+01, 2.3552200e+07],\n",
       "       [1.6100000e+01, 1.6129999e+01, 1.6000000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 1.9011500e+07],\n",
       "       [1.6100000e+01, 1.6100000e+01, 1.6100000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 0.0000000e+00]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_train = base_train_csv.iloc[:, 1:7].values\n",
    "base_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = MinMaxScaler(feature_range=(0, 1))\n",
    "base_train_ = normalizer.fit_transform(base_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevs_normalizer = MinMaxScaler(feature_range=(0, 1))\n",
    "base_train_ = prevs_normalizer.fit_transform(base_train[:, 0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938],\n",
       "       [0.7562984 ],\n",
       "       [0.78149225],\n",
       "       ...,\n",
       "       [0.57122093],\n",
       "       [0.57655039],\n",
       "       [0.57655039]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1242, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "antecipator = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [] # Previsores\n",
    "y_train = [] # Preços reais\n",
    "for i in range(antecipator, base_train_.shape[0]): # 90 preços anteriores para prever intervalo atual\n",
    "    x_train.append(base_train_[i - antecipator:i, 0:base_train_.shape[1]])\n",
    "    y_train.append(base_train_[i, 0])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.76501938],\n",
       "        [0.7562984 ],\n",
       "        [0.78149225],\n",
       "        ...,\n",
       "        [0.74273261],\n",
       "        [0.74127907],\n",
       "        [0.74224806]],\n",
       "\n",
       "       [[0.7562984 ],\n",
       "        [0.78149225],\n",
       "        [0.78875969],\n",
       "        ...,\n",
       "        [0.74127907],\n",
       "        [0.74224806],\n",
       "        [0.76114341]],\n",
       "\n",
       "       [[0.78149225],\n",
       "        [0.78875969],\n",
       "        [0.77083338],\n",
       "        ...,\n",
       "        [0.74224806],\n",
       "        [0.76114341],\n",
       "        [0.76114341]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.46414729],\n",
       "        [0.46414729],\n",
       "        [0.46850775],\n",
       "        ...,\n",
       "        [0.55959302],\n",
       "        [0.55959302],\n",
       "        [0.55959302]],\n",
       "\n",
       "       [[0.46414729],\n",
       "        [0.46850775],\n",
       "        [0.47141473],\n",
       "        ...,\n",
       "        [0.55959302],\n",
       "        [0.55959302],\n",
       "        [0.57122093]],\n",
       "\n",
       "       [[0.46850775],\n",
       "        [0.47141473],\n",
       "        [0.46317829],\n",
       "        ...,\n",
       "        [0.55959302],\n",
       "        [0.57122093],\n",
       "        [0.57655039]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 90, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input((x_train.shape[1], x_train.shape[2])))\n",
    "\n",
    "model.add(LSTM(units=100, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "          \n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "          \n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m40,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m30,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">111,451</span> (435.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m111,451\u001b[0m (435.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">111,451</span> (435.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m111,451\u001b[0m (435.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    min_delta=1e-10,\n",
    "    patience=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_plateau = ReduceLROnPlateau(\n",
    "    monitor='loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='checkpoints/weights_prev_one.keras',\n",
    "    monitor='loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621ms/step - loss: 0.0545 - mean_absolute_error: 0.1763\n",
      "Epoch 1: loss improved from inf to 0.02975, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 632ms/step - loss: 0.0538 - mean_absolute_error: 0.1751 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - loss: 0.0123 - mean_absolute_error: 0.0835\n",
      "Epoch 2: loss improved from 0.02975 to 0.01392, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 232ms/step - loss: 0.0124 - mean_absolute_error: 0.0836 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - loss: 0.0131 - mean_absolute_error: 0.0915\n",
      "Epoch 3: loss improved from 0.01392 to 0.01199, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 293ms/step - loss: 0.0131 - mean_absolute_error: 0.0913 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - loss: 0.0112 - mean_absolute_error: 0.0804\n",
      "Epoch 4: loss improved from 0.01199 to 0.01052, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 428ms/step - loss: 0.0112 - mean_absolute_error: 0.0804 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522ms/step - loss: 0.0110 - mean_absolute_error: 0.0809\n",
      "Epoch 5: loss improved from 0.01052 to 0.00978, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 526ms/step - loss: 0.0109 - mean_absolute_error: 0.0807 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561ms/step - loss: 0.0105 - mean_absolute_error: 0.0784\n",
      "Epoch 6: loss improved from 0.00978 to 0.00919, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 567ms/step - loss: 0.0105 - mean_absolute_error: 0.0783 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495ms/step - loss: 0.0087 - mean_absolute_error: 0.0719\n",
      "Epoch 7: loss did not improve from 0.00919\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 496ms/step - loss: 0.0087 - mean_absolute_error: 0.0720 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step - loss: 0.0066 - mean_absolute_error: 0.0618\n",
      "Epoch 8: loss improved from 0.00919 to 0.00749, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 374ms/step - loss: 0.0066 - mean_absolute_error: 0.0619 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 0.0080 - mean_absolute_error: 0.0691\n",
      "Epoch 9: loss did not improve from 0.00749\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 179ms/step - loss: 0.0080 - mean_absolute_error: 0.0691 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 0.0073 - mean_absolute_error: 0.0660\n",
      "Epoch 10: loss improved from 0.00749 to 0.00705, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 196ms/step - loss: 0.0073 - mean_absolute_error: 0.0659 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - loss: 0.0069 - mean_absolute_error: 0.0638\n",
      "Epoch 11: loss improved from 0.00705 to 0.00646, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 197ms/step - loss: 0.0069 - mean_absolute_error: 0.0638 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - loss: 0.0066 - mean_absolute_error: 0.0626\n",
      "Epoch 12: loss improved from 0.00646 to 0.00639, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 198ms/step - loss: 0.0066 - mean_absolute_error: 0.0625 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.0060 - mean_absolute_error: 0.0581\n",
      "Epoch 13: loss improved from 0.00639 to 0.00609, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 213ms/step - loss: 0.0060 - mean_absolute_error: 0.0581 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - loss: 0.0057 - mean_absolute_error: 0.0577\n",
      "Epoch 14: loss improved from 0.00609 to 0.00603, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 209ms/step - loss: 0.0057 - mean_absolute_error: 0.0577 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step - loss: 0.0060 - mean_absolute_error: 0.0589\n",
      "Epoch 15: loss improved from 0.00603 to 0.00540, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 339ms/step - loss: 0.0059 - mean_absolute_error: 0.0589 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - loss: 0.0051 - mean_absolute_error: 0.0542\n",
      "Epoch 16: loss improved from 0.00540 to 0.00532, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 227ms/step - loss: 0.0051 - mean_absolute_error: 0.0542 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - loss: 0.0053 - mean_absolute_error: 0.0560\n",
      "Epoch 17: loss did not improve from 0.00532\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 246ms/step - loss: 0.0053 - mean_absolute_error: 0.0560 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - loss: 0.0046 - mean_absolute_error: 0.0526\n",
      "Epoch 18: loss improved from 0.00532 to 0.00495, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 276ms/step - loss: 0.0046 - mean_absolute_error: 0.0526 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step - loss: 0.0044 - mean_absolute_error: 0.0515\n",
      "Epoch 19: loss improved from 0.00495 to 0.00473, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 381ms/step - loss: 0.0044 - mean_absolute_error: 0.0516 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517ms/step - loss: 0.0052 - mean_absolute_error: 0.0539\n",
      "Epoch 20: loss did not improve from 0.00473\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 518ms/step - loss: 0.0052 - mean_absolute_error: 0.0538 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - loss: 0.0048 - mean_absolute_error: 0.0518\n",
      "Epoch 21: loss did not improve from 0.00473\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 251ms/step - loss: 0.0048 - mean_absolute_error: 0.0519 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - loss: 0.0045 - mean_absolute_error: 0.0511\n",
      "Epoch 22: loss improved from 0.00473 to 0.00463, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 401ms/step - loss: 0.0045 - mean_absolute_error: 0.0511 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - loss: 0.0047 - mean_absolute_error: 0.0531\n",
      "Epoch 23: loss improved from 0.00463 to 0.00453, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 292ms/step - loss: 0.0047 - mean_absolute_error: 0.0530 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - loss: 0.0044 - mean_absolute_error: 0.0497\n",
      "Epoch 24: loss improved from 0.00453 to 0.00419, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 293ms/step - loss: 0.0044 - mean_absolute_error: 0.0497 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step - loss: 0.0045 - mean_absolute_error: 0.0507\n",
      "Epoch 25: loss did not improve from 0.00419\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 376ms/step - loss: 0.0045 - mean_absolute_error: 0.0507 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - loss: 0.0039 - mean_absolute_error: 0.0486\n",
      "Epoch 26: loss improved from 0.00419 to 0.00388, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 220ms/step - loss: 0.0039 - mean_absolute_error: 0.0486 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - loss: 0.0047 - mean_absolute_error: 0.0513\n",
      "Epoch 27: loss did not improve from 0.00388\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 246ms/step - loss: 0.0047 - mean_absolute_error: 0.0512 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - loss: 0.0039 - mean_absolute_error: 0.0476\n",
      "Epoch 28: loss improved from 0.00388 to 0.00386, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 331ms/step - loss: 0.0039 - mean_absolute_error: 0.0476 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579ms/step - loss: 0.0039 - mean_absolute_error: 0.0478\n",
      "Epoch 29: loss did not improve from 0.00386\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 582ms/step - loss: 0.0039 - mean_absolute_error: 0.0478 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - loss: 0.0040 - mean_absolute_error: 0.0478\n",
      "Epoch 30: loss improved from 0.00386 to 0.00363, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 403ms/step - loss: 0.0040 - mean_absolute_error: 0.0477 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - loss: 0.0038 - mean_absolute_error: 0.0466\n",
      "Epoch 31: loss improved from 0.00363 to 0.00358, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 338ms/step - loss: 0.0038 - mean_absolute_error: 0.0466 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - loss: 0.0039 - mean_absolute_error: 0.0470\n",
      "Epoch 32: loss did not improve from 0.00358\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 328ms/step - loss: 0.0039 - mean_absolute_error: 0.0469 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - loss: 0.0034 - mean_absolute_error: 0.0445\n",
      "Epoch 33: loss did not improve from 0.00358\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 247ms/step - loss: 0.0034 - mean_absolute_error: 0.0446 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 0.0035 - mean_absolute_error: 0.0453\n",
      "Epoch 34: loss improved from 0.00358 to 0.00343, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 187ms/step - loss: 0.0035 - mean_absolute_error: 0.0453 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - loss: 0.0032 - mean_absolute_error: 0.0437\n",
      "Epoch 35: loss did not improve from 0.00343\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 210ms/step - loss: 0.0032 - mean_absolute_error: 0.0438 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - loss: 0.0037 - mean_absolute_error: 0.0454\n",
      "Epoch 36: loss did not improve from 0.00343\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 270ms/step - loss: 0.0037 - mean_absolute_error: 0.0454 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453ms/step - loss: 0.0035 - mean_absolute_error: 0.0451\n",
      "Epoch 37: loss improved from 0.00343 to 0.00340, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 457ms/step - loss: 0.0035 - mean_absolute_error: 0.0451 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580ms/step - loss: 0.0029 - mean_absolute_error: 0.0407\n",
      "Epoch 38: loss improved from 0.00340 to 0.00319, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 585ms/step - loss: 0.0029 - mean_absolute_error: 0.0408 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - loss: 0.0032 - mean_absolute_error: 0.0426\n",
      "Epoch 39: loss improved from 0.00319 to 0.00317, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 406ms/step - loss: 0.0032 - mean_absolute_error: 0.0426 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - loss: 0.0028 - mean_absolute_error: 0.0400\n",
      "Epoch 40: loss improved from 0.00317 to 0.00290, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 252ms/step - loss: 0.0028 - mean_absolute_error: 0.0400 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 0.0027 - mean_absolute_error: 0.0408\n",
      "Epoch 41: loss did not improve from 0.00290\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 183ms/step - loss: 0.0027 - mean_absolute_error: 0.0408 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - loss: 0.0031 - mean_absolute_error: 0.0427\n",
      "Epoch 42: loss did not improve from 0.00290\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 175ms/step - loss: 0.0031 - mean_absolute_error: 0.0427 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - loss: 0.0031 - mean_absolute_error: 0.0418\n",
      "Epoch 43: loss did not improve from 0.00290\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 328ms/step - loss: 0.0031 - mean_absolute_error: 0.0418 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - loss: 0.0028 - mean_absolute_error: 0.0407\n",
      "Epoch 44: loss improved from 0.00290 to 0.00280, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 201ms/step - loss: 0.0028 - mean_absolute_error: 0.0406 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - loss: 0.0028 - mean_absolute_error: 0.0402\n",
      "Epoch 45: loss improved from 0.00280 to 0.00275, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 185ms/step - loss: 0.0028 - mean_absolute_error: 0.0402 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - loss: 0.0031 - mean_absolute_error: 0.0424\n",
      "Epoch 46: loss did not improve from 0.00275\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 214ms/step - loss: 0.0031 - mean_absolute_error: 0.0424 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - loss: 0.0028 - mean_absolute_error: 0.0406\n",
      "Epoch 47: loss did not improve from 0.00275\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 248ms/step - loss: 0.0028 - mean_absolute_error: 0.0406 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - loss: 0.0028 - mean_absolute_error: 0.0400\n",
      "Epoch 48: loss did not improve from 0.00275\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 251ms/step - loss: 0.0028 - mean_absolute_error: 0.0400 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - loss: 0.0031 - mean_absolute_error: 0.0421\n",
      "Epoch 49: loss did not improve from 0.00275\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 250ms/step - loss: 0.0031 - mean_absolute_error: 0.0421 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - loss: 0.0030 - mean_absolute_error: 0.0410\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 50: loss did not improve from 0.00275\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 270ms/step - loss: 0.0030 - mean_absolute_error: 0.0410 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - loss: 0.0024 - mean_absolute_error: 0.0375\n",
      "Epoch 51: loss improved from 0.00275 to 0.00237, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 283ms/step - loss: 0.0024 - mean_absolute_error: 0.0375 - learning_rate: 2.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step - loss: 0.0024 - mean_absolute_error: 0.0374\n",
      "Epoch 52: loss improved from 0.00237 to 0.00229, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 304ms/step - loss: 0.0024 - mean_absolute_error: 0.0374 - learning_rate: 2.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - loss: 0.0022 - mean_absolute_error: 0.0355\n",
      "Epoch 53: loss improved from 0.00229 to 0.00221, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 215ms/step - loss: 0.0022 - mean_absolute_error: 0.0355 - learning_rate: 2.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 0.0022 - mean_absolute_error: 0.0347\n",
      "Epoch 54: loss did not improve from 0.00221\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 185ms/step - loss: 0.0022 - mean_absolute_error: 0.0347 - learning_rate: 2.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 0.0023 - mean_absolute_error: 0.0362\n",
      "Epoch 55: loss did not improve from 0.00221\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 171ms/step - loss: 0.0023 - mean_absolute_error: 0.0362 - learning_rate: 2.0000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - loss: 0.0023 - mean_absolute_error: 0.0362\n",
      "Epoch 56: loss did not improve from 0.00221\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 171ms/step - loss: 0.0023 - mean_absolute_error: 0.0362 - learning_rate: 2.0000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 0.0023 - mean_absolute_error: 0.0365\n",
      "Epoch 57: loss did not improve from 0.00221\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 171ms/step - loss: 0.0023 - mean_absolute_error: 0.0365 - learning_rate: 2.0000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - loss: 0.0019 - mean_absolute_error: 0.0336\n",
      "Epoch 58: loss improved from 0.00221 to 0.00201, saving model to checkpoints/weights_prev_one.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 172ms/step - loss: 0.0019 - mean_absolute_error: 0.0337 - learning_rate: 2.0000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - loss: 0.0021 - mean_absolute_error: 0.0347\n",
      "Epoch 59: loss did not improve from 0.00201\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 177ms/step - loss: 0.0021 - mean_absolute_error: 0.0347 - learning_rate: 2.0000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - loss: 0.0025 - mean_absolute_error: 0.0376\n",
      "Epoch 60: loss did not improve from 0.00201\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 171ms/step - loss: 0.0025 - mean_absolute_error: 0.0375 - learning_rate: 2.0000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - loss: 0.0023 - mean_absolute_error: 0.0358\n",
      "Epoch 61: loss did not improve from 0.00201\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 169ms/step - loss: 0.0023 - mean_absolute_error: 0.0358 - learning_rate: 2.0000e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - loss: 0.0021 - mean_absolute_error: 0.0346\n",
      "Epoch 62: loss did not improve from 0.00201\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 216ms/step - loss: 0.0021 - mean_absolute_error: 0.0346 - learning_rate: 2.0000e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - loss: 0.0023 - mean_absolute_error: 0.0366\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 63: loss did not improve from 0.00201\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 271ms/step - loss: 0.0023 - mean_absolute_error: 0.0365 - learning_rate: 2.0000e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step - loss: 0.0020 - mean_absolute_error: 0.0337\n",
      "Epoch 64: loss did not improve from 0.00201\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 375ms/step - loss: 0.0020 - mean_absolute_error: 0.0337 - learning_rate: 4.0000e-05\n",
      "Epoch 65/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - loss: 0.0025 - mean_absolute_error: 0.0374\n",
      "Epoch 65: loss did not improve from 0.00201\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 290ms/step - loss: 0.0025 - mean_absolute_error: 0.0374 - learning_rate: 4.0000e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - loss: 0.0021 - mean_absolute_error: 0.0343\n",
      "Epoch 66: loss did not improve from 0.00201\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 225ms/step - loss: 0.0021 - mean_absolute_error: 0.0343 - learning_rate: 4.0000e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - loss: 0.0024 - mean_absolute_error: 0.0360\n",
      "Epoch 67: loss did not improve from 0.00201\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 174ms/step - loss: 0.0024 - mean_absolute_error: 0.0360 - learning_rate: 4.0000e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - loss: 0.0022 - mean_absolute_error: 0.0346\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 68: loss did not improve from 0.00201\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 420ms/step - loss: 0.0022 - mean_absolute_error: 0.0346 - learning_rate: 4.0000e-05\n",
      "Epoch 68: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x779bad5a0820>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, batch_size=32, callbacks=[early_stop, reduce_plateau, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.190001],\n",
       "       [16.49    ],\n",
       "       [16.780001],\n",
       "       [16.700001],\n",
       "       [16.74    ],\n",
       "       [17.030001],\n",
       "       [16.92    ],\n",
       "       [16.879999],\n",
       "       [17.040001],\n",
       "       [17.32    ],\n",
       "       [17.35    ],\n",
       "       [17.92    ],\n",
       "       [18.35    ],\n",
       "       [18.309999],\n",
       "       [18.26    ],\n",
       "       [18.4     ],\n",
       "       [18.42    ],\n",
       "       [19.34    ],\n",
       "       [19.620001],\n",
       "       [19.67    ],\n",
       "       [19.77    ],\n",
       "       [19.74    ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = base_test_csv.iloc[:, 1:2].values\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[            Date       Open       High        Low      Close  Adj Close  \\\n",
       " 0     2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       " 1     2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       " 2     2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       " 3     2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       " 4     2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       " ...          ...        ...        ...        ...        ...        ...   \n",
       " 1240  2017-12-25  15.750000  15.750000  15.750000  15.750000  15.718563   \n",
       " 1241  2017-12-26  15.750000  15.990000  15.690000  15.970000  15.938125   \n",
       " 1242  2017-12-27  15.990000  16.139999  15.980000  16.049999  16.017963   \n",
       " 1243  2017-12-28  16.100000  16.129999  16.000000  16.100000  16.067865   \n",
       " 1244  2017-12-29  16.100000  16.100000  16.100000  16.100000  16.067865   \n",
       " \n",
       "           Volume  \n",
       " 0     30182600.0  \n",
       " 1     30552600.0  \n",
       " 2     36141000.0  \n",
       " 3     28069600.0  \n",
       " 4     29091300.0  \n",
       " ...          ...  \n",
       " 1240         0.0  \n",
       " 1241  22173100.0  \n",
       " 1242  23552200.0  \n",
       " 1243  19011500.0  \n",
       " 1244         0.0  \n",
       " \n",
       " [1242 rows x 7 columns],\n",
       "           Date       Open       High        Low      Close  Adj Close  \\\n",
       " 0   2018-01-02  16.190001  16.549999  16.190001  16.549999  16.516966   \n",
       " 1   2018-01-03  16.490000  16.719999  16.370001  16.700001  16.666668   \n",
       " 2   2018-01-04  16.780001  16.959999  16.620001  16.730000  16.696608   \n",
       " 3   2018-01-05  16.700001  16.860001  16.570000  16.830000  16.796408   \n",
       " 4   2018-01-08  16.740000  17.030001  16.709999  17.030001  16.996010   \n",
       " 5   2018-01-09  17.030001  17.160000  16.959999  17.030001  16.996010   \n",
       " 6   2018-01-10  16.920000  17.049999  16.770000  16.799999  16.766466   \n",
       " 7   2018-01-11  16.879999  17.299999  16.840000  17.250000  17.215569   \n",
       " 8   2018-01-12  17.040001  17.410000  17.020000  17.299999  17.265469   \n",
       " 9   2018-01-15  17.320000  17.440001  17.150000  17.350000  17.315371   \n",
       " 10  2018-01-16  17.350000  17.840000  17.299999  17.650000  17.614771   \n",
       " 11  2018-01-17  17.920000  18.360001  17.809999  18.360001  18.323355   \n",
       " 12  2018-01-18  18.350000  18.530001  17.930000  18.219999  18.183632   \n",
       " 13  2018-01-19  18.309999  18.420000  18.030001  18.260000  18.223553   \n",
       " 14  2018-01-22  18.260000  18.469999  18.090000  18.469999  18.433134   \n",
       " 15  2018-01-23  18.400000  18.459999  18.000000  18.240000  18.203592   \n",
       " 16  2018-01-24  18.420000  19.629999  18.420000  19.340000  19.301397   \n",
       " 17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       " 18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       " 19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       " 20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       " 21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       " \n",
       "       Volume  \n",
       " 0   33461800  \n",
       " 1   55940900  \n",
       " 2   37064900  \n",
       " 3   26958200  \n",
       " 4   28400000  \n",
       " 5   35070900  \n",
       " 6   28547700  \n",
       " 7   37921500  \n",
       " 8   45912100  \n",
       " 9   28945400  \n",
       " 10  58618300  \n",
       " 11  58488900  \n",
       " 12  48575800  \n",
       " 13  33470200  \n",
       " 14  33920000  \n",
       " 15  35567700  \n",
       " 16  89768200  \n",
       " 17         0  \n",
       " 18  81989500  \n",
       " 19  55726200  \n",
       " 20  46203000  \n",
       " 21  41576600  ]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [base_train_csv, base_test_csv]\n",
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>19.620001</td>\n",
       "      <td>19.980000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.890221</td>\n",
       "      <td>81989500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>19.670000</td>\n",
       "      <td>20.049999</td>\n",
       "      <td>19.570000</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>19.810381</td>\n",
       "      <td>55726200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.360001</td>\n",
       "      <td>19.490000</td>\n",
       "      <td>19.451097</td>\n",
       "      <td>46203000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>19.740000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.680000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>19.660681</td>\n",
       "      <td>41576600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1264 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Open       High        Low      Close  Adj Close  \\\n",
       "0   2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1   2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2   2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3   2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4   2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       "18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       "19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       "20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       "21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       "\n",
       "        Volume  \n",
       "0   30182600.0  \n",
       "1   30552600.0  \n",
       "2   36141000.0  \n",
       "3   28069600.0  \n",
       "4   29091300.0  \n",
       "..         ...  \n",
       "17         0.0  \n",
       "18  81989500.0  \n",
       "19  55726200.0  \n",
       "20  46203000.0  \n",
       "21  41576600.0  \n",
       "\n",
       "[1264 rows x 7 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_complete = pd.concat(frames)\n",
    "base_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_complete = base_complete.drop('Date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3930000e+01, 1.4030000e+01, 1.3760000e+01, 1.3870000e+01,\n",
       "        1.3842316e+01, 2.7208100e+07],\n",
       "       [1.3760000e+01, 1.3850000e+01, 1.3680000e+01, 1.3850000e+01,\n",
       "        1.3822356e+01, 2.7306400e+07],\n",
       "       [1.3790000e+01, 1.3900000e+01, 1.3440000e+01, 1.3450000e+01,\n",
       "        1.3423154e+01, 5.8871700e+07],\n",
       "       [1.3530000e+01, 1.3770000e+01, 1.3470000e+01, 1.3650000e+01,\n",
       "        1.3622754e+01, 8.2909400e+07],\n",
       "       [1.3850000e+01, 1.4190000e+01, 1.3820000e+01, 1.4020000e+01,\n",
       "        1.3992017e+01, 6.0260300e+07],\n",
       "       [1.3960000e+01, 1.4180000e+01, 1.3940000e+01, 1.4170000e+01,\n",
       "        1.4141717e+01, 1.8139300e+07],\n",
       "       [1.4570000e+01, 1.4650000e+01, 1.4230000e+01, 1.4410000e+01,\n",
       "        1.4381238e+01, 5.6476800e+07],\n",
       "       [1.4650000e+01, 1.5020000e+01, 1.4510000e+01, 1.5020000e+01,\n",
       "        1.4990021e+01, 6.8418200e+07],\n",
       "       [1.5020000e+01, 1.5020000e+01, 1.5020000e+01, 1.5020000e+01,\n",
       "        1.4990021e+01, 0.0000000e+00],\n",
       "       [1.5100000e+01, 1.5150000e+01, 1.4690000e+01, 1.4710000e+01,\n",
       "        1.4680639e+01, 3.6337400e+07],\n",
       "       [1.4880000e+01, 1.5050000e+01, 1.4810000e+01, 1.4990000e+01,\n",
       "        1.4960080e+01, 3.4915900e+07],\n",
       "       [1.4980000e+01, 1.5160000e+01, 1.4860000e+01, 1.4870000e+01,\n",
       "        1.4840320e+01, 4.9702800e+07],\n",
       "       [1.4940000e+01, 1.5100000e+01, 1.4810000e+01, 1.5030000e+01,\n",
       "        1.5000000e+01, 3.7010200e+07],\n",
       "       [1.5030000e+01, 1.5260000e+01, 1.5020000e+01, 1.5040000e+01,\n",
       "        1.5009980e+01, 3.4413800e+07],\n",
       "       [1.5070000e+01, 1.5170000e+01, 1.4990000e+01, 1.5040000e+01,\n",
       "        1.5009980e+01, 4.7784700e+07],\n",
       "       [1.5020000e+01, 1.5190000e+01, 1.4980000e+01, 1.5040000e+01,\n",
       "        1.5009980e+01, 4.7601200e+07],\n",
       "       [1.5100000e+01, 1.5170000e+01, 1.4920000e+01, 1.5140000e+01,\n",
       "        1.5109781e+01, 3.5822100e+07],\n",
       "       [1.5250000e+01, 1.5880000e+01, 1.5070000e+01, 1.5870000e+01,\n",
       "        1.5838324e+01, 8.0267000e+07],\n",
       "       [1.5850000e+01, 1.5960000e+01, 1.5580000e+01, 1.5670000e+01,\n",
       "        1.5638723e+01, 4.6258800e+07],\n",
       "       [1.5600000e+01, 1.5800000e+01, 1.5430000e+01, 1.5690000e+01,\n",
       "        1.5658683e+01, 4.0928300e+07],\n",
       "       [1.5790000e+01, 1.5960000e+01, 1.5700000e+01, 1.5840000e+01,\n",
       "        1.5808384e+01, 3.6733200e+07],\n",
       "       [1.5860000e+01, 1.5900000e+01, 1.5560000e+01, 1.5560000e+01,\n",
       "        1.5528943e+01, 3.7874200e+07],\n",
       "       [1.5700000e+01, 1.5720000e+01, 1.5110000e+01, 1.5310000e+01,\n",
       "        1.5279442e+01, 4.1819300e+07],\n",
       "       [1.5370000e+01, 1.5500000e+01, 1.5220000e+01, 1.5340000e+01,\n",
       "        1.5309381e+01, 3.3829000e+07],\n",
       "       [1.5500000e+01, 1.5520000e+01, 1.5300000e+01, 1.5300000e+01,\n",
       "        1.5269462e+01, 2.8638300e+07],\n",
       "       [1.5190000e+01, 1.5400000e+01, 1.5060000e+01, 1.5400000e+01,\n",
       "        1.5369262e+01, 2.9826200e+07],\n",
       "       [1.5600000e+01, 1.5980000e+01, 1.5520000e+01, 1.5980000e+01,\n",
       "        1.5948104e+01, 5.0636700e+07],\n",
       "       [1.5900000e+01, 1.5940000e+01, 1.5650000e+01, 1.5660000e+01,\n",
       "        1.5628743e+01, 4.7798600e+07],\n",
       "       [1.5880000e+01, 1.6110001e+01, 1.5850000e+01, 1.5900000e+01,\n",
       "        1.5868263e+01, 5.5361300e+07],\n",
       "       [1.5660000e+01, 1.5770000e+01, 1.5540000e+01, 1.5690000e+01,\n",
       "        1.5658683e+01, 4.1741300e+07],\n",
       "       [1.5610000e+01, 1.5890000e+01, 1.5590000e+01, 1.5890000e+01,\n",
       "        1.5858284e+01, 2.7904700e+07],\n",
       "       [1.6129999e+01, 1.6190001e+01, 1.6010000e+01, 1.6190001e+01,\n",
       "        1.6157686e+01, 4.7066600e+07],\n",
       "       [1.6170000e+01, 1.6250000e+01, 1.6010000e+01, 1.6080000e+01,\n",
       "        1.6047905e+01, 4.0422100e+07],\n",
       "       [1.6080000e+01, 1.6080000e+01, 1.6080000e+01, 1.6080000e+01,\n",
       "        1.6047905e+01, 0.0000000e+00],\n",
       "       [1.6230000e+01, 1.6290001e+01, 1.6059999e+01, 1.6080000e+01,\n",
       "        1.6047905e+01, 2.4210000e+07],\n",
       "       [1.6160000e+01, 1.6260000e+01, 1.6000000e+01, 1.6120001e+01,\n",
       "        1.6087826e+01, 4.4699700e+07],\n",
       "       [1.6139999e+01, 1.6219999e+01, 1.6070000e+01, 1.6129999e+01,\n",
       "        1.6097803e+01, 2.5524800e+07],\n",
       "       [1.6219999e+01, 1.6280001e+01, 1.6129999e+01, 1.6160000e+01,\n",
       "        1.6127745e+01, 2.5706200e+07],\n",
       "       [1.6000000e+01, 1.6160000e+01, 1.5900000e+01, 1.6150000e+01,\n",
       "        1.6117765e+01, 2.4672800e+07],\n",
       "       [1.6190001e+01, 1.6389999e+01, 1.6170000e+01, 1.6219999e+01,\n",
       "        1.6187624e+01, 3.2417500e+07],\n",
       "       [1.6290001e+01, 1.6290001e+01, 1.6120001e+01, 1.6200001e+01,\n",
       "        1.6167665e+01, 2.9389900e+07],\n",
       "       [1.6290001e+01, 1.6510000e+01, 1.6120001e+01, 1.6510000e+01,\n",
       "        1.6477047e+01, 4.6249500e+07],\n",
       "       [1.6530001e+01, 1.6730000e+01, 1.6450001e+01, 1.6719999e+01,\n",
       "        1.6686626e+01, 3.7608200e+07],\n",
       "       [1.6780001e+01, 1.6889999e+01, 1.6660000e+01, 1.6730000e+01,\n",
       "        1.6696608e+01, 3.7848300e+07],\n",
       "       [1.6770000e+01, 1.7090000e+01, 1.6650000e+01, 1.7030001e+01,\n",
       "        1.6996010e+01, 4.5640100e+07],\n",
       "       [1.6969999e+01, 1.7170000e+01, 1.6740000e+01, 1.6780001e+01,\n",
       "        1.6746508e+01, 5.5355600e+07],\n",
       "       [1.6900000e+01, 1.6950001e+01, 1.6719999e+01, 1.6770000e+01,\n",
       "        1.6736528e+01, 3.2249000e+07],\n",
       "       [1.6990000e+01, 1.7100000e+01, 1.6879999e+01, 1.6900000e+01,\n",
       "        1.6866268e+01, 3.8876600e+07],\n",
       "       [1.6900000e+01, 1.6900000e+01, 1.6900000e+01, 1.6900000e+01,\n",
       "        1.6866268e+01, 0.0000000e+00],\n",
       "       [1.6959999e+01, 1.7010000e+01, 1.6680000e+01, 1.6940001e+01,\n",
       "        1.6906189e+01, 3.2605400e+07],\n",
       "       [1.7049999e+01, 1.7440001e+01, 1.6980000e+01, 1.7430000e+01,\n",
       "        1.7395210e+01, 4.6056100e+07],\n",
       "       [1.7309999e+01, 1.7350000e+01, 1.6500000e+01, 1.6500000e+01,\n",
       "        1.6467066e+01, 6.1098400e+07],\n",
       "       [1.6690001e+01, 1.6950001e+01, 1.6510000e+01, 1.6950001e+01,\n",
       "        1.6916168e+01, 4.1179600e+07],\n",
       "       [1.6889999e+01, 1.6940001e+01, 1.6719999e+01, 1.6719999e+01,\n",
       "        1.6686626e+01, 2.9399400e+07],\n",
       "       [1.6709999e+01, 1.6809999e+01, 1.6510000e+01, 1.6719999e+01,\n",
       "        1.6686626e+01, 3.5959400e+07],\n",
       "       [1.6690001e+01, 1.6770000e+01, 1.6389999e+01, 1.6639999e+01,\n",
       "        1.6606787e+01, 2.8697700e+07],\n",
       "       [1.6639999e+01, 1.6639999e+01, 1.5280000e+01, 1.5350000e+01,\n",
       "        1.5319362e+01, 8.8765600e+07],\n",
       "       [1.5350000e+01, 1.5350000e+01, 1.5350000e+01, 1.5350000e+01,\n",
       "        1.5319362e+01, 0.0000000e+00],\n",
       "       [1.5620000e+01, 1.6040001e+01, 1.5480000e+01, 1.5810000e+01,\n",
       "        1.5778444e+01, 4.2703800e+07],\n",
       "       [1.5920000e+01, 1.6120001e+01, 1.5810000e+01, 1.6020000e+01,\n",
       "        1.5988025e+01, 3.8376900e+07],\n",
       "       [1.6020000e+01, 1.6020000e+01, 1.6020000e+01, 1.6020000e+01,\n",
       "        1.5988025e+01, 0.0000000e+00],\n",
       "       [1.6150000e+01, 1.6309999e+01, 1.5850000e+01, 1.5900000e+01,\n",
       "        1.5868263e+01, 4.5817800e+07],\n",
       "       [1.6090000e+01, 1.6240000e+01, 1.5930000e+01, 1.6110001e+01,\n",
       "        1.6077845e+01, 3.7444900e+07],\n",
       "       [1.5980000e+01, 1.6260000e+01, 1.5940000e+01, 1.6190001e+01,\n",
       "        1.6157686e+01, 1.5403600e+07],\n",
       "       [1.6250000e+01, 1.6370001e+01, 1.6040001e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 1.8790700e+07],\n",
       "       [1.6010000e+01, 1.6020000e+01, 1.5780000e+01, 1.5870000e+01,\n",
       "        1.5838324e+01, 2.8445800e+07],\n",
       "       [1.5930000e+01, 1.6040001e+01, 1.5810000e+01, 1.5840000e+01,\n",
       "        1.5808384e+01, 3.0429600e+07],\n",
       "       [1.5870000e+01, 1.5920000e+01, 1.5320000e+01, 1.5330000e+01,\n",
       "        1.5299401e+01, 4.5973000e+07],\n",
       "       [1.5300000e+01, 1.5470000e+01, 1.4990000e+01, 1.5380000e+01,\n",
       "        1.5349302e+01, 5.2811400e+07],\n",
       "       [1.5340000e+01, 1.5770000e+01, 1.5260000e+01, 1.5610000e+01,\n",
       "        1.5578842e+01, 4.2703800e+07],\n",
       "       [1.5650000e+01, 1.5800000e+01, 1.5460000e+01, 1.5480000e+01,\n",
       "        1.5449101e+01, 4.3821500e+07],\n",
       "       [1.5500000e+01, 1.5830000e+01, 1.5210000e+01, 1.5310000e+01,\n",
       "        1.5279442e+01, 3.0228000e+07],\n",
       "       [1.5220000e+01, 1.5700000e+01, 1.5140000e+01, 1.5520000e+01,\n",
       "        1.5489023e+01, 3.9238500e+07],\n",
       "       [1.5300000e+01, 1.5490000e+01, 1.5070000e+01, 1.5260000e+01,\n",
       "        1.5229542e+01, 3.7281400e+07],\n",
       "       [1.5510000e+01, 1.5680000e+01, 1.5350000e+01, 1.5350000e+01,\n",
       "        1.5319362e+01, 3.9584500e+07],\n",
       "       [1.5480000e+01, 1.5570000e+01, 1.5370000e+01, 1.5380000e+01,\n",
       "        1.5349302e+01, 2.1281600e+07],\n",
       "       [1.5360000e+01, 1.5490000e+01, 1.5180000e+01, 1.5490000e+01,\n",
       "        1.5459082e+01, 3.6201200e+07],\n",
       "       [1.5650000e+01, 1.5680000e+01, 1.5110000e+01, 1.5180000e+01,\n",
       "        1.5149701e+01, 4.6828900e+07],\n",
       "       [1.5100000e+01, 1.5310000e+01, 1.5000000e+01, 1.5010000e+01,\n",
       "        1.4980041e+01, 3.7177300e+07],\n",
       "       [1.5050000e+01, 1.5240000e+01, 1.4950000e+01, 1.4950000e+01,\n",
       "        1.4920160e+01, 5.5668300e+07],\n",
       "       [1.5160000e+01, 1.5330000e+01, 1.5130000e+01, 1.5220000e+01,\n",
       "        1.5189621e+01, 4.2760400e+07],\n",
       "       [1.5180000e+01, 1.5250000e+01, 1.5060000e+01, 1.5140000e+01,\n",
       "        1.5109781e+01, 2.2639700e+07],\n",
       "       [1.5210000e+01, 1.5300000e+01, 1.5170000e+01, 1.5240000e+01,\n",
       "        1.5209581e+01, 2.0149700e+07],\n",
       "       [1.5310000e+01, 1.5870000e+01, 1.5300000e+01, 1.5860000e+01,\n",
       "        1.5828343e+01, 4.7219400e+07],\n",
       "       [1.5750000e+01, 1.5890000e+01, 1.5690000e+01, 1.5750000e+01,\n",
       "        1.5718563e+01, 1.8708500e+07],\n",
       "       [1.5750000e+01, 1.5750000e+01, 1.5750000e+01, 1.5750000e+01,\n",
       "        1.5718563e+01, 0.0000000e+00],\n",
       "       [1.5750000e+01, 1.5990000e+01, 1.5690000e+01, 1.5970000e+01,\n",
       "        1.5938125e+01, 2.2173100e+07],\n",
       "       [1.5990000e+01, 1.6139999e+01, 1.5980000e+01, 1.6049999e+01,\n",
       "        1.6017963e+01, 2.3552200e+07],\n",
       "       [1.6100000e+01, 1.6129999e+01, 1.6000000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 1.9011500e+07],\n",
       "       [1.6100000e+01, 1.6100000e+01, 1.6100000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 0.0000000e+00],\n",
       "       [1.6190001e+01, 1.6549999e+01, 1.6190001e+01, 1.6549999e+01,\n",
       "        1.6516966e+01, 3.3461800e+07],\n",
       "       [1.6490000e+01, 1.6719999e+01, 1.6370001e+01, 1.6700001e+01,\n",
       "        1.6666668e+01, 5.5940900e+07],\n",
       "       [1.6780001e+01, 1.6959999e+01, 1.6620001e+01, 1.6730000e+01,\n",
       "        1.6696608e+01, 3.7064900e+07],\n",
       "       [1.6700001e+01, 1.6860001e+01, 1.6570000e+01, 1.6830000e+01,\n",
       "        1.6796408e+01, 2.6958200e+07],\n",
       "       [1.6740000e+01, 1.7030001e+01, 1.6709999e+01, 1.7030001e+01,\n",
       "        1.6996010e+01, 2.8400000e+07],\n",
       "       [1.7030001e+01, 1.7160000e+01, 1.6959999e+01, 1.7030001e+01,\n",
       "        1.6996010e+01, 3.5070900e+07],\n",
       "       [1.6920000e+01, 1.7049999e+01, 1.6770000e+01, 1.6799999e+01,\n",
       "        1.6766466e+01, 2.8547700e+07],\n",
       "       [1.6879999e+01, 1.7299999e+01, 1.6840000e+01, 1.7250000e+01,\n",
       "        1.7215569e+01, 3.7921500e+07],\n",
       "       [1.7040001e+01, 1.7410000e+01, 1.7020000e+01, 1.7299999e+01,\n",
       "        1.7265469e+01, 4.5912100e+07],\n",
       "       [1.7320000e+01, 1.7440001e+01, 1.7150000e+01, 1.7350000e+01,\n",
       "        1.7315371e+01, 2.8945400e+07],\n",
       "       [1.7350000e+01, 1.7840000e+01, 1.7299999e+01, 1.7650000e+01,\n",
       "        1.7614771e+01, 5.8618300e+07],\n",
       "       [1.7920000e+01, 1.8360001e+01, 1.7809999e+01, 1.8360001e+01,\n",
       "        1.8323355e+01, 5.8488900e+07],\n",
       "       [1.8350000e+01, 1.8530001e+01, 1.7930000e+01, 1.8219999e+01,\n",
       "        1.8183632e+01, 4.8575800e+07],\n",
       "       [1.8309999e+01, 1.8420000e+01, 1.8030001e+01, 1.8260000e+01,\n",
       "        1.8223553e+01, 3.3470200e+07],\n",
       "       [1.8260000e+01, 1.8469999e+01, 1.8090000e+01, 1.8469999e+01,\n",
       "        1.8433134e+01, 3.3920000e+07],\n",
       "       [1.8400000e+01, 1.8459999e+01, 1.8000000e+01, 1.8240000e+01,\n",
       "        1.8203592e+01, 3.5567700e+07],\n",
       "       [1.8420000e+01, 1.9629999e+01, 1.8420000e+01, 1.9340000e+01,\n",
       "        1.9301397e+01, 8.9768200e+07],\n",
       "       [1.9340000e+01, 1.9340000e+01, 1.9340000e+01, 1.9340000e+01,\n",
       "        1.9301397e+01, 0.0000000e+00],\n",
       "       [1.9620001e+01, 1.9980000e+01, 1.9100000e+01, 1.9930000e+01,\n",
       "        1.9890221e+01, 8.1989500e+07],\n",
       "       [1.9670000e+01, 2.0049999e+01, 1.9570000e+01, 1.9850000e+01,\n",
       "        1.9810381e+01, 5.5726200e+07],\n",
       "       [1.9770000e+01, 1.9770000e+01, 1.9360001e+01, 1.9490000e+01,\n",
       "        1.9451097e+01, 4.6203000e+07],\n",
       "       [1.9740000e+01, 1.9930000e+01, 1.9680000e+01, 1.9700001e+01,\n",
       "        1.9660681e+01, 4.1576600e+07]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = base_complete[len(base_complete) - len(base_test_csv) - antecipator:].values\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47141473, 0.47309743, 0.49334698, 0.47495091, 0.47495089,\n",
       "        0.03892707],\n",
       "       [0.46317829, 0.46437227, 0.48925281, 0.47396859, 0.47396857,\n",
       "        0.03906771],\n",
       "       [0.46463178, 0.46679593, 0.47697032, 0.45432222, 0.45432219,\n",
       "        0.0842287 ],\n",
       "       [0.45203488, 0.46049443, 0.47850563, 0.46414541, 0.46414533,\n",
       "        0.11861983],\n",
       "       [0.46753876, 0.48085313, 0.4964176 , 0.48231829, 0.48231829,\n",
       "        0.08621539],\n",
       "       [0.47286822, 0.4803684 , 0.50255885, 0.48968568, 0.48968565,\n",
       "        0.02595219],\n",
       "       [0.50242248, 0.50315075, 0.5174002 , 0.5014735 , 0.50147347,\n",
       "        0.08080228],\n",
       "       [0.50629845, 0.5210858 , 0.53172979, 0.53143421, 0.53143421,\n",
       "        0.09788703],\n",
       "       [0.52422481, 0.5210858 , 0.55783009, 0.53143421, 0.53143421,\n",
       "        0.        ],\n",
       "       [0.52810078, 0.5273873 , 0.54094166, 0.51620828, 0.51620824,\n",
       "        0.05198851],\n",
       "       [0.51744186, 0.52253999, 0.54708291, 0.52996073, 0.52996068,\n",
       "        0.04995475],\n",
       "       [0.52228682, 0.52787203, 0.54964176, 0.52406682, 0.5240668 ,\n",
       "        0.0711106 ],\n",
       "       [0.52034884, 0.52496365, 0.54708291, 0.53192537, 0.53192531,\n",
       "        0.05295109],\n",
       "       [0.5247093 , 0.53271934, 0.55783009, 0.53241653, 0.53241647,\n",
       "        0.04923638],\n",
       "       [0.52664729, 0.52835676, 0.55629478, 0.53241653, 0.53241647,\n",
       "        0.06836635],\n",
       "       [0.52422481, 0.52932622, 0.55578301, 0.53241653, 0.53241647,\n",
       "        0.06810381],\n",
       "       [0.52810078, 0.52835676, 0.55271238, 0.53732812, 0.53732809,\n",
       "        0.05125126],\n",
       "       [0.53536822, 0.56277266, 0.56038895, 0.57318274, 0.57318271,\n",
       "        0.1148393 ],\n",
       "       [0.56443798, 0.56665051, 0.58648925, 0.56335956, 0.56335952,\n",
       "        0.06618322],\n",
       "       [0.55232558, 0.55889481, 0.57881269, 0.56434187, 0.56434183,\n",
       "        0.05855678],\n",
       "       [0.56153101, 0.56665051, 0.5926305 , 0.57170926, 0.57170924,\n",
       "        0.05255479],\n",
       "       [0.56492248, 0.56374212, 0.58546571, 0.55795681, 0.55795679,\n",
       "        0.05418723],\n",
       "       [0.55717054, 0.55501697, 0.56243603, 0.54567783, 0.54567781,\n",
       "        0.05983155],\n",
       "       [0.54118217, 0.54435288, 0.56806551, 0.5471513 , 0.54715123,\n",
       "        0.0483997 ],\n",
       "       [0.54748062, 0.54532235, 0.57215967, 0.54518667, 0.54518665,\n",
       "        0.04097328],\n",
       "       [0.53246124, 0.53950557, 0.55987718, 0.55009826, 0.55009823,\n",
       "        0.04267283],\n",
       "       [0.55232558, 0.56761997, 0.58341863, 0.57858549, 0.57858544,\n",
       "        0.07244675],\n",
       "       [0.56686047, 0.56568105, 0.59007165, 0.5628684 , 0.56286836,\n",
       "        0.06838623],\n",
       "       [0.56589147, 0.57392152, 0.60030706, 0.57465622, 0.57465613,\n",
       "        0.07920631],\n",
       "       [0.55523256, 0.55744062, 0.58444217, 0.56434187, 0.56434183,\n",
       "        0.05971996],\n",
       "       [0.55281008, 0.56325739, 0.58700102, 0.57416506, 0.57416503,\n",
       "        0.03992371],\n",
       "       [0.57800383, 0.57779937, 0.60849539, 0.58889988, 0.58889984,\n",
       "        0.06733895],\n",
       "       [0.57994186, 0.58070771, 0.60849539, 0.58349708, 0.58349706,\n",
       "        0.05783256],\n",
       "       [0.5755814 , 0.57246728, 0.61207779, 0.58349708, 0.58349706,\n",
       "        0.        ],\n",
       "       [0.58284884, 0.58264668, 0.6110542 , 0.58349708, 0.58349706,\n",
       "        0.03463764],\n",
       "       [0.57945736, 0.58119244, 0.60798362, 0.58546177, 0.58546174,\n",
       "        0.06395259],\n",
       "       [0.57848832, 0.57925347, 0.61156602, 0.58595283, 0.58595275,\n",
       "        0.03651875],\n",
       "       [0.58236429, 0.58216195, 0.61463659, 0.58742635, 0.58742632,\n",
       "        0.03677828],\n",
       "       [0.57170543, 0.57634513, 0.60286592, 0.5869352 , 0.58693516,\n",
       "        0.03529978],\n",
       "       [0.5809109 , 0.58749389, 0.61668373, 0.59037326, 0.59037321,\n",
       "        0.04638024],\n",
       "       [0.58575586, 0.58264668, 0.61412492, 0.58939104, 0.58939095,\n",
       "        0.04204861],\n",
       "       [0.58575586, 0.59331071, 0.61412492, 0.60461693, 0.60461692,\n",
       "        0.06616991],\n",
       "       [0.59738377, 0.60397479, 0.63101336, 0.61493122, 0.61493117,\n",
       "        0.05380666],\n",
       "       [0.60949617, 0.61173044, 0.64176049, 0.61542243, 0.61542242,\n",
       "        0.05415018],\n",
       "       [0.60901163, 0.62142511, 0.64124872, 0.63015725, 0.63015724,\n",
       "        0.06529803],\n",
       "       [0.6187015 , 0.62530296, 0.64585466, 0.61787827, 0.61787821,\n",
       "        0.07919816],\n",
       "       [0.61531008, 0.61463892, 0.64483106, 0.61738706, 0.61738705,\n",
       "        0.04613917],\n",
       "       [0.61967054, 0.62190984, 0.6530194 , 0.62377213, 0.62377209,\n",
       "        0.05562138],\n",
       "       [0.61531008, 0.61221522, 0.65404299, 0.62377213, 0.62377209,\n",
       "        0.        ],\n",
       "       [0.61821701, 0.61754726, 0.64278403, 0.62573682, 0.62573677,\n",
       "        0.04664908],\n",
       "       [0.62257747, 0.63839074, 0.65813715, 0.64980357, 0.64980352,\n",
       "        0.06589321],\n",
       "       [0.63517437, 0.63402811, 0.63357216, 0.60412577, 0.60412571,\n",
       "        0.08741447],\n",
       "       [0.60513571, 0.61463892, 0.63408393, 0.62622798, 0.62622788,\n",
       "        0.05891632],\n",
       "       [0.61482553, 0.61415419, 0.64483106, 0.61493122, 0.61493117,\n",
       "        0.0420622 ],\n",
       "       [0.6061046 , 0.60785259, 0.63408393, 0.61493122, 0.61493117,\n",
       "        0.0514477 ],\n",
       "       [0.60513571, 0.60591372, 0.62794263, 0.61100195, 0.61100196,\n",
       "        0.04105827],\n",
       "       [0.60271313, 0.59961217, 0.57113613, 0.54764246, 0.54764244,\n",
       "        0.12699839],\n",
       "       [0.54021318, 0.53708192, 0.57471853, 0.54764246, 0.54764244,\n",
       "        0.        ],\n",
       "       [0.55329457, 0.57052841, 0.58137155, 0.57023578, 0.57023577,\n",
       "        0.06109702],\n",
       "       [0.56782946, 0.57440625, 0.59825998, 0.58055013, 0.58055012,\n",
       "        0.05490645],\n",
       "       [0.57267442, 0.56955889, 0.60900716, 0.58055013, 0.58055012,\n",
       "        0.        ],\n",
       "       [0.57897287, 0.58361604, 0.60030706, 0.57465622, 0.57465613,\n",
       "        0.06555227],\n",
       "       [0.57606589, 0.58022298, 0.60440123, 0.58497061, 0.58497053,\n",
       "        0.05357303],\n",
       "       [0.57073643, 0.58119244, 0.604913  , 0.58889988, 0.58889984,\n",
       "        0.02203818],\n",
       "       [0.58381783, 0.58652453, 0.61003076, 0.5844794 , 0.58447937,\n",
       "        0.02688416],\n",
       "       [0.57218992, 0.56955889, 0.59672467, 0.57318274, 0.57318271,\n",
       "        0.04069787],\n",
       "       [0.56831395, 0.57052841, 0.59825998, 0.57170926, 0.57170924,\n",
       "        0.04353612],\n",
       "       [0.56540698, 0.56471159, 0.57318321, 0.54666014, 0.54666008,\n",
       "        0.06577432],\n",
       "       [0.5377907 , 0.54289869, 0.55629478, 0.54911594, 0.54911591,\n",
       "        0.07555813],\n",
       "       [0.53972868, 0.55744062, 0.57011259, 0.5604126 , 0.56041253,\n",
       "        0.06109702],\n",
       "       [0.55474806, 0.55889481, 0.580348  , 0.55402753, 0.55402743,\n",
       "        0.06269613],\n",
       "       [0.54748062, 0.56034901, 0.56755374, 0.54567783, 0.54567781,\n",
       "        0.04324769],\n",
       "       [0.53391473, 0.5540475 , 0.56397134, 0.55599217, 0.55599216,\n",
       "        0.05613916],\n",
       "       [0.5377907 , 0.54386815, 0.56038895, 0.54322203, 0.54322203,\n",
       "        0.0533391 ],\n",
       "       [0.54796512, 0.55307804, 0.57471853, 0.54764246, 0.54764244,\n",
       "        0.05663419],\n",
       "       [0.54651163, 0.547746  , 0.57574207, 0.54911594, 0.54911591,\n",
       "        0.03044793],\n",
       "       [0.54069767, 0.54386815, 0.56601842, 0.55451869, 0.55451864,\n",
       "        0.05179365],\n",
       "       [0.55474806, 0.55307804, 0.56243603, 0.53929276, 0.53929272,\n",
       "        0.06699887],\n",
       "       [0.52810078, 0.535143  , 0.55680655, 0.53094305, 0.53094305,\n",
       "        0.05319017],\n",
       "       [0.52567829, 0.53174988, 0.5542477 , 0.5279961 , 0.52799606,\n",
       "        0.07964554],\n",
       "       [0.53100775, 0.53611246, 0.56345957, 0.54125739, 0.54125735,\n",
       "        0.061178  ],\n",
       "       [0.53197674, 0.53223461, 0.55987718, 0.53732812, 0.53732809,\n",
       "        0.03239099],\n",
       "       [0.53343023, 0.53465826, 0.56550665, 0.54223971, 0.54223966,\n",
       "        0.0288285 ],\n",
       "       [0.53827519, 0.56228793, 0.57215967, 0.57269158, 0.5726915 ,\n",
       "        0.06755756],\n",
       "       [0.55959302, 0.56325739, 0.59211873, 0.56728883, 0.56728878,\n",
       "        0.02676656],\n",
       "       [0.55959302, 0.55647116, 0.59518936, 0.56728883, 0.56728878,\n",
       "        0.        ],\n",
       "       [0.55959302, 0.5681047 , 0.59211873, 0.57809433, 0.57809433,\n",
       "        0.03172341],\n",
       "       [0.57122093, 0.57537562, 0.60696008, 0.58202356, 0.58202349,\n",
       "        0.03369652],\n",
       "       [0.57655039, 0.57489089, 0.60798362, 0.5844794 , 0.58447937,\n",
       "        0.02720006],\n",
       "       [0.57655039, 0.57343674, 0.61310133, 0.5844794 , 0.58447937,\n",
       "        0.        ],\n",
       "       [0.5809109 , 0.59524959, 0.61770732, 0.60658151, 0.6065815 ,\n",
       "        0.04787434],\n",
       "       [0.59544574, 0.60349001, 0.62691919, 0.613949  , 0.61394895,\n",
       "        0.08003555],\n",
       "       [0.60949617, 0.61512356, 0.63971346, 0.61542243, 0.61542242,\n",
       "        0.05302935],\n",
       "       [0.6056202 , 0.61027635, 0.63715455, 0.62033402, 0.62033399,\n",
       "        0.03856953],\n",
       "       [0.60755814, 0.61851677, 0.64431929, 0.63015725, 0.63015724,\n",
       "        0.04063234],\n",
       "       [0.62160858, 0.62481823, 0.65711356, 0.63015725, 0.63015724,\n",
       "        0.05017651],\n",
       "       [0.61627907, 0.61948614, 0.64738997, 0.61886049, 0.61886042,\n",
       "        0.04084366],\n",
       "       [0.61434104, 0.63160441, 0.65097236, 0.6409627 , 0.64096264,\n",
       "        0.05425491],\n",
       "       [0.62209307, 0.6369365 , 0.66018424, 0.64341845, 0.64341843,\n",
       "        0.06568719],\n",
       "       [0.63565891, 0.63839074, 0.66683726, 0.64587429, 0.64587431,\n",
       "        0.04141265],\n",
       "       [0.6371124 , 0.65777993, 0.67451377, 0.66060907, 0.66060903,\n",
       "        0.08386615],\n",
       "       [0.66472868, 0.68298599, 0.70061407, 0.69548142, 0.69548138,\n",
       "        0.08368102],\n",
       "       [0.68556202, 0.69122642, 0.70675537, 0.68860509, 0.68860504,\n",
       "        0.06949819],\n",
       "       [0.68362398, 0.68589433, 0.71187313, 0.69056978, 0.69056971,\n",
       "        0.04788636],\n",
       "       [0.68120155, 0.68831794, 0.71494371, 0.70088407, 0.70088406,\n",
       "        0.0485299 ],\n",
       "       [0.6879845 , 0.6878332 , 0.71033777, 0.68958746, 0.68958735,\n",
       "        0.05088729],\n",
       "       [0.68895349, 0.74454673, 0.73183214, 0.74361497, 0.74361488,\n",
       "        0.12843282],\n",
       "       [0.73352713, 0.73048958, 0.77891505, 0.74361497, 0.74361488,\n",
       "        0.        ],\n",
       "       [0.74709307, 0.76151236, 0.76663255, 0.77259336, 0.77259335,\n",
       "        0.11730371],\n",
       "       [0.7495155 , 0.76490543, 0.79068577, 0.76866408, 0.76866409,\n",
       "        0.07972838],\n",
       "       [0.75436047, 0.75133301, 0.77993864, 0.75098236, 0.75098224,\n",
       "        0.06610338],\n",
       "       [0.75290698, 0.75908871, 0.79631525, 0.76129675, 0.76129674,\n",
       "        0.05948432]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = normalizer.transform(input)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.47141473, 0.47309743, 0.49334698, 0.47495091, 0.47495089,\n",
       "         0.03892707],\n",
       "        [0.46317829, 0.46437227, 0.48925281, 0.47396859, 0.47396857,\n",
       "         0.03906771],\n",
       "        [0.46463178, 0.46679593, 0.47697032, 0.45432222, 0.45432219,\n",
       "         0.0842287 ],\n",
       "        ...,\n",
       "        [0.57122093, 0.57537562, 0.60696008, 0.58202356, 0.58202349,\n",
       "         0.03369652],\n",
       "        [0.57655039, 0.57489089, 0.60798362, 0.5844794 , 0.58447937,\n",
       "         0.02720006],\n",
       "        [0.57655039, 0.57343674, 0.61310133, 0.5844794 , 0.58447937,\n",
       "         0.        ]],\n",
       "\n",
       "       [[0.46317829, 0.46437227, 0.48925281, 0.47396859, 0.47396857,\n",
       "         0.03906771],\n",
       "        [0.46463178, 0.46679593, 0.47697032, 0.45432222, 0.45432219,\n",
       "         0.0842287 ],\n",
       "        [0.45203488, 0.46049443, 0.47850563, 0.46414541, 0.46414533,\n",
       "         0.11861983],\n",
       "        ...,\n",
       "        [0.57655039, 0.57489089, 0.60798362, 0.5844794 , 0.58447937,\n",
       "         0.02720006],\n",
       "        [0.57655039, 0.57343674, 0.61310133, 0.5844794 , 0.58447937,\n",
       "         0.        ],\n",
       "        [0.5809109 , 0.59524959, 0.61770732, 0.60658151, 0.6065815 ,\n",
       "         0.04787434]],\n",
       "\n",
       "       [[0.46463178, 0.46679593, 0.47697032, 0.45432222, 0.45432219,\n",
       "         0.0842287 ],\n",
       "        [0.45203488, 0.46049443, 0.47850563, 0.46414541, 0.46414533,\n",
       "         0.11861983],\n",
       "        [0.46753876, 0.48085313, 0.4964176 , 0.48231829, 0.48231829,\n",
       "         0.08621539],\n",
       "        ...,\n",
       "        [0.57655039, 0.57343674, 0.61310133, 0.5844794 , 0.58447937,\n",
       "         0.        ],\n",
       "        [0.5809109 , 0.59524959, 0.61770732, 0.60658151, 0.6065815 ,\n",
       "         0.04787434],\n",
       "        [0.59544574, 0.60349001, 0.62691919, 0.613949  , 0.61394895,\n",
       "         0.08003555]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.55232558, 0.55889481, 0.57881269, 0.56434187, 0.56434183,\n",
       "         0.05855678],\n",
       "        [0.56153101, 0.56665051, 0.5926305 , 0.57170926, 0.57170924,\n",
       "         0.05255479],\n",
       "        [0.56492248, 0.56374212, 0.58546571, 0.55795681, 0.55795679,\n",
       "         0.05418723],\n",
       "        ...,\n",
       "        [0.68895349, 0.74454673, 0.73183214, 0.74361497, 0.74361488,\n",
       "         0.12843282],\n",
       "        [0.73352713, 0.73048958, 0.77891505, 0.74361497, 0.74361488,\n",
       "         0.        ],\n",
       "        [0.74709307, 0.76151236, 0.76663255, 0.77259336, 0.77259335,\n",
       "         0.11730371]],\n",
       "\n",
       "       [[0.56153101, 0.56665051, 0.5926305 , 0.57170926, 0.57170924,\n",
       "         0.05255479],\n",
       "        [0.56492248, 0.56374212, 0.58546571, 0.55795681, 0.55795679,\n",
       "         0.05418723],\n",
       "        [0.55717054, 0.55501697, 0.56243603, 0.54567783, 0.54567781,\n",
       "         0.05983155],\n",
       "        ...,\n",
       "        [0.73352713, 0.73048958, 0.77891505, 0.74361497, 0.74361488,\n",
       "         0.        ],\n",
       "        [0.74709307, 0.76151236, 0.76663255, 0.77259336, 0.77259335,\n",
       "         0.11730371],\n",
       "        [0.7495155 , 0.76490543, 0.79068577, 0.76866408, 0.76866409,\n",
       "         0.07972838]],\n",
       "\n",
       "       [[0.56492248, 0.56374212, 0.58546571, 0.55795681, 0.55795679,\n",
       "         0.05418723],\n",
       "        [0.55717054, 0.55501697, 0.56243603, 0.54567783, 0.54567781,\n",
       "         0.05983155],\n",
       "        [0.54118217, 0.54435288, 0.56806551, 0.5471513 , 0.54715123,\n",
       "         0.0483997 ],\n",
       "        ...,\n",
       "        [0.74709307, 0.76151236, 0.76663255, 0.77259336, 0.77259335,\n",
       "         0.11730371],\n",
       "        [0.7495155 , 0.76490543, 0.79068577, 0.76866408, 0.76866409,\n",
       "         0.07972838],\n",
       "        [0.75436047, 0.75133301, 0.77993864, 0.75098236, 0.75098224,\n",
       "         0.06610338]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = [input[i-antecipator:i, 0:6] for i in range(antecipator, input.shape[0])]\n",
    "x_test = np.array(x_test)\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 90, 6)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling LSTMCell.call().\n\n\u001b[1mDimensions must be equal, but are 6 and 1 for '{{node sequential_1/lstm_1/lstm_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_1/lstm_1/strided_slice_1, sequential_1/lstm_1/lstm_cell_1/Cast/ReadVariableOp)' with input shapes: [22,6], [1,400].\u001b[0m\n\nArguments received by LSTMCell.call():\n  • inputs=tf.Tensor(shape=(22, 6), dtype=float32)\n  • states=('tf.Tensor(shape=(22, 100), dtype=float32)', 'tf.Tensor(shape=(22, 100), dtype=float32)')\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prevs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m prevs\n",
      "File \u001b[0;32m~/anaconda3/envs/dlp/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/dlp/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling LSTMCell.call().\n\n\u001b[1mDimensions must be equal, but are 6 and 1 for '{{node sequential_1/lstm_1/lstm_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_1/lstm_1/strided_slice_1, sequential_1/lstm_1/lstm_cell_1/Cast/ReadVariableOp)' with input shapes: [22,6], [1,400].\u001b[0m\n\nArguments received by LSTMCell.call():\n  • inputs=tf.Tensor(shape=(22, 6), dtype=float32)\n  • states=('tf.Tensor(shape=(22, 100), dtype=float32)', 'tf.Tensor(shape=(22, 100), dtype=float32)')\n  • training=False"
     ]
    }
   ],
   "source": [
    "prevs = model.predict(x_test)\n",
    "prevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.06155 ],\n",
       "       [16.18891 ],\n",
       "       [16.309292],\n",
       "       [16.430826],\n",
       "       [16.551216],\n",
       "       [16.67031 ],\n",
       "       [16.786053],\n",
       "       [16.8803  ],\n",
       "       [16.957146],\n",
       "       [17.023739],\n",
       "       [17.091032],\n",
       "       [17.168617],\n",
       "       [17.291317],\n",
       "       [17.463486],\n",
       "       [17.663248],\n",
       "       [17.865534],\n",
       "       [18.038857],\n",
       "       [18.205372],\n",
       "       [18.404268],\n",
       "       [18.642569],\n",
       "       [18.900913],\n",
       "       [19.131927]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevs = prevs_normalizer.inverse_transform(prevs)\n",
    "prevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.351204, 17.87454563636364)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevs.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5233693890769264"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(prevs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_test, color='red', label='Preço Real')\n",
    "plt.plot(prevs, color='blue', label='Previsões')\n",
    "plt.title('Previsão dos preços das ações')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multiplas Saídas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train = base_train_csv.iloc[:, 1:2].values\n",
    "base_value_max = base_train_csv.iloc[:, 2:3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.809999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.110001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>15.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>15.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>15.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>16.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>16.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1242 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0     19.990000\n",
       "1     19.809999\n",
       "2     20.330000\n",
       "3     20.480000\n",
       "4     20.110001\n",
       "...         ...\n",
       "1237  15.750000\n",
       "1238  15.750000\n",
       "1239  15.990000\n",
       "1240  16.100000\n",
       "1241  16.100000\n",
       "\n",
       "[1242 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(base_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.209999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.620001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>15.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>15.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>16.139999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>16.129999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>16.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1242 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0     20.209999\n",
       "1     20.400000\n",
       "2     20.620001\n",
       "3     20.670000\n",
       "4     20.230000\n",
       "...         ...\n",
       "1237  15.750000\n",
       "1238  15.990000\n",
       "1239  16.139999\n",
       "1240  16.129999\n",
       "1241  16.100000\n",
       "\n",
       "[1242 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(base_value_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = MinMaxScaler(feature_range=(0, 1))\n",
    "base_train = normalizer.fit_transform(base_train)\n",
    "base_value_max = normalizer.fit_transform(base_value_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.765019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.756298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.781492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.788760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>0.559593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>0.559593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>0.571221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>0.576550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>0.576550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1242 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0     0.765019\n",
       "1     0.756298\n",
       "2     0.781492\n",
       "3     0.788760\n",
       "4     0.770833\n",
       "...        ...\n",
       "1237  0.559593\n",
       "1238  0.559593\n",
       "1239  0.571221\n",
       "1240  0.576550\n",
       "1241  0.576550\n",
       "\n",
       "[1242 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(base_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.772661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.781871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.792535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.794959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.773631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>0.556471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>0.568105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>0.575376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>0.574891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>0.573437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1242 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0     0.772661\n",
       "1     0.781871\n",
       "2     0.792535\n",
       "3     0.794959\n",
       "4     0.773631\n",
       "...        ...\n",
       "1237  0.556471\n",
       "1238  0.568105\n",
       "1239  0.575376\n",
       "1240  0.574891\n",
       "1241  0.573437\n",
       "\n",
       "[1242 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(base_value_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "antec = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1242, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train1 = []\n",
    "y_train2 = []\n",
    "\n",
    "for i in range(antec, base_train.shape[0]):\n",
    "    x_train.append(base_train[i - antec:i, 0])\n",
    "    y_train1.append(base_train[i, 0])\n",
    "    y_train2.append(base_value_max[i, 0])\n",
    "\n",
    "x_train, y_train1, y_train2 = np.array(x_train), np.array(y_train1), np.array(y_train2)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 90, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76114341, 0.76490543],\n",
       "       [0.76114341, 0.7746001 ],\n",
       "       [0.77470935, 0.78090155],\n",
       "       ...,\n",
       "       [0.57122093, 0.57537562],\n",
       "       [0.57655039, 0.57489089],\n",
       "       [0.57655039, 0.57343674]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.column_stack((y_train1, y_train2))\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input((x_train.shape[1], x_train.shape[2])))\n",
    "\n",
    "model.add(LSTM(units=100, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "          \n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "          \n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=2, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='checkpoints/weights_prev_multiple.keras',\n",
    "    monitor='loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - loss: 0.0657 - mean_absolute_error: 0.1871\n",
      "Epoch 1: loss improved from inf to 0.03420, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 199ms/step - loss: 0.0649 - mean_absolute_error: 0.1857 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - loss: 0.0206 - mean_absolute_error: 0.1128\n",
      "Epoch 2: loss improved from 0.03420 to 0.01842, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 208ms/step - loss: 0.0206 - mean_absolute_error: 0.1126 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.0139 - mean_absolute_error: 0.0921\n",
      "Epoch 3: loss improved from 0.01842 to 0.01354, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 213ms/step - loss: 0.0139 - mean_absolute_error: 0.0920 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - loss: 0.0136 - mean_absolute_error: 0.0896\n",
      "Epoch 4: loss improved from 0.01354 to 0.01272, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 264ms/step - loss: 0.0135 - mean_absolute_error: 0.0895 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - loss: 0.0129 - mean_absolute_error: 0.0875\n",
      "Epoch 5: loss improved from 0.01272 to 0.01211, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 278ms/step - loss: 0.0129 - mean_absolute_error: 0.0874 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - loss: 0.0114 - mean_absolute_error: 0.0819\n",
      "Epoch 6: loss improved from 0.01211 to 0.01142, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 251ms/step - loss: 0.0114 - mean_absolute_error: 0.0819 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - loss: 0.0099 - mean_absolute_error: 0.0754\n",
      "Epoch 7: loss improved from 0.01142 to 0.00982, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 204ms/step - loss: 0.0099 - mean_absolute_error: 0.0754 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - loss: 0.0099 - mean_absolute_error: 0.0753\n",
      "Epoch 8: loss did not improve from 0.00982\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 194ms/step - loss: 0.0099 - mean_absolute_error: 0.0753 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - loss: 0.0090 - mean_absolute_error: 0.0725\n",
      "Epoch 9: loss improved from 0.00982 to 0.00846, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 205ms/step - loss: 0.0090 - mean_absolute_error: 0.0725 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 0.0081 - mean_absolute_error: 0.0690\n",
      "Epoch 10: loss improved from 0.00846 to 0.00792, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 200ms/step - loss: 0.0081 - mean_absolute_error: 0.0690 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - loss: 0.0095 - mean_absolute_error: 0.0724\n",
      "Epoch 11: loss did not improve from 0.00792\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 268ms/step - loss: 0.0094 - mean_absolute_error: 0.0723 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - loss: 0.0085 - mean_absolute_error: 0.0702\n",
      "Epoch 12: loss did not improve from 0.00792\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 262ms/step - loss: 0.0085 - mean_absolute_error: 0.0701 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - loss: 0.0070 - mean_absolute_error: 0.0630\n",
      "Epoch 13: loss improved from 0.00792 to 0.00693, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 265ms/step - loss: 0.0070 - mean_absolute_error: 0.0630 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - loss: 0.0060 - mean_absolute_error: 0.0588\n",
      "Epoch 14: loss improved from 0.00693 to 0.00678, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 254ms/step - loss: 0.0061 - mean_absolute_error: 0.0589 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - loss: 0.0061 - mean_absolute_error: 0.0576\n",
      "Epoch 15: loss improved from 0.00678 to 0.00628, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 261ms/step - loss: 0.0062 - mean_absolute_error: 0.0577 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - loss: 0.0066 - mean_absolute_error: 0.0623\n",
      "Epoch 16: loss improved from 0.00628 to 0.00595, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 267ms/step - loss: 0.0066 - mean_absolute_error: 0.0622 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - loss: 0.0056 - mean_absolute_error: 0.0563\n",
      "Epoch 17: loss improved from 0.00595 to 0.00553, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 266ms/step - loss: 0.0056 - mean_absolute_error: 0.0564 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - loss: 0.0055 - mean_absolute_error: 0.0561\n",
      "Epoch 18: loss improved from 0.00553 to 0.00521, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 256ms/step - loss: 0.0055 - mean_absolute_error: 0.0561 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - loss: 0.0053 - mean_absolute_error: 0.0560\n",
      "Epoch 19: loss did not improve from 0.00521\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 252ms/step - loss: 0.0053 - mean_absolute_error: 0.0560 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - loss: 0.0054 - mean_absolute_error: 0.0546\n",
      "Epoch 20: loss did not improve from 0.00521\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 262ms/step - loss: 0.0054 - mean_absolute_error: 0.0546 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - loss: 0.0043 - mean_absolute_error: 0.0497\n",
      "Epoch 21: loss improved from 0.00521 to 0.00468, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 263ms/step - loss: 0.0043 - mean_absolute_error: 0.0498 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - loss: 0.0050 - mean_absolute_error: 0.0533\n",
      "Epoch 22: loss did not improve from 0.00468\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 285ms/step - loss: 0.0050 - mean_absolute_error: 0.0533 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 0.0049 - mean_absolute_error: 0.0525\n",
      "Epoch 23: loss did not improve from 0.00468\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 268ms/step - loss: 0.0049 - mean_absolute_error: 0.0525 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - loss: 0.0047 - mean_absolute_error: 0.0519\n",
      "Epoch 24: loss improved from 0.00468 to 0.00460, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 218ms/step - loss: 0.0047 - mean_absolute_error: 0.0519 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - loss: 0.0045 - mean_absolute_error: 0.0502\n",
      "Epoch 25: loss improved from 0.00460 to 0.00449, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 204ms/step - loss: 0.0045 - mean_absolute_error: 0.0502 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - loss: 0.0040 - mean_absolute_error: 0.0475\n",
      "Epoch 26: loss improved from 0.00449 to 0.00419, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 217ms/step - loss: 0.0040 - mean_absolute_error: 0.0476 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - loss: 0.0040 - mean_absolute_error: 0.0483\n",
      "Epoch 27: loss improved from 0.00419 to 0.00402, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 229ms/step - loss: 0.0040 - mean_absolute_error: 0.0483 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - loss: 0.0042 - mean_absolute_error: 0.0491\n",
      "Epoch 28: loss did not improve from 0.00402\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 221ms/step - loss: 0.0042 - mean_absolute_error: 0.0491 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - loss: 0.0038 - mean_absolute_error: 0.0476\n",
      "Epoch 29: loss improved from 0.00402 to 0.00395, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 223ms/step - loss: 0.0038 - mean_absolute_error: 0.0476 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.0039 - mean_absolute_error: 0.0476\n",
      "Epoch 30: loss improved from 0.00395 to 0.00388, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 214ms/step - loss: 0.0039 - mean_absolute_error: 0.0476 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - loss: 0.0037 - mean_absolute_error: 0.0456\n",
      "Epoch 31: loss did not improve from 0.00388\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 199ms/step - loss: 0.0037 - mean_absolute_error: 0.0457 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - loss: 0.0035 - mean_absolute_error: 0.0447\n",
      "Epoch 32: loss improved from 0.00388 to 0.00345, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 204ms/step - loss: 0.0035 - mean_absolute_error: 0.0447 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - loss: 0.0036 - mean_absolute_error: 0.0454\n",
      "Epoch 33: loss did not improve from 0.00345\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 209ms/step - loss: 0.0036 - mean_absolute_error: 0.0454 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - loss: 0.0038 - mean_absolute_error: 0.0464\n",
      "Epoch 34: loss did not improve from 0.00345\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 214ms/step - loss: 0.0038 - mean_absolute_error: 0.0464 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - loss: 0.0036 - mean_absolute_error: 0.0451\n",
      "Epoch 35: loss improved from 0.00345 to 0.00328, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 222ms/step - loss: 0.0036 - mean_absolute_error: 0.0450 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - loss: 0.0033 - mean_absolute_error: 0.0444\n",
      "Epoch 36: loss did not improve from 0.00328\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 203ms/step - loss: 0.0033 - mean_absolute_error: 0.0444 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - loss: 0.0035 - mean_absolute_error: 0.0445\n",
      "Epoch 37: loss improved from 0.00328 to 0.00319, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 201ms/step - loss: 0.0035 - mean_absolute_error: 0.0445 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 0.0030 - mean_absolute_error: 0.0412\n",
      "Epoch 38: loss improved from 0.00319 to 0.00306, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 202ms/step - loss: 0.0030 - mean_absolute_error: 0.0413 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - loss: 0.0032 - mean_absolute_error: 0.0443\n",
      "Epoch 39: loss did not improve from 0.00306\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 198ms/step - loss: 0.0032 - mean_absolute_error: 0.0443 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - loss: 0.0025 - mean_absolute_error: 0.0383\n",
      "Epoch 40: loss improved from 0.00306 to 0.00291, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 204ms/step - loss: 0.0025 - mean_absolute_error: 0.0384 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - loss: 0.0029 - mean_absolute_error: 0.0411\n",
      "Epoch 41: loss did not improve from 0.00291\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 196ms/step - loss: 0.0029 - mean_absolute_error: 0.0411 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 0.0029 - mean_absolute_error: 0.0405\n",
      "Epoch 42: loss did not improve from 0.00291\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 197ms/step - loss: 0.0029 - mean_absolute_error: 0.0406 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - loss: 0.0031 - mean_absolute_error: 0.0421\n",
      "Epoch 43: loss did not improve from 0.00291\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 200ms/step - loss: 0.0031 - mean_absolute_error: 0.0421 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 0.0031 - mean_absolute_error: 0.0429\n",
      "Epoch 44: loss did not improve from 0.00291\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 201ms/step - loss: 0.0031 - mean_absolute_error: 0.0429 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - loss: 0.0028 - mean_absolute_error: 0.0405\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 45: loss did not improve from 0.00291\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 205ms/step - loss: 0.0028 - mean_absolute_error: 0.0406 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - loss: 0.0029 - mean_absolute_error: 0.0394\n",
      "Epoch 46: loss improved from 0.00291 to 0.00257, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 205ms/step - loss: 0.0029 - mean_absolute_error: 0.0394 - learning_rate: 2.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - loss: 0.0025 - mean_absolute_error: 0.0380\n",
      "Epoch 47: loss improved from 0.00257 to 0.00247, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 209ms/step - loss: 0.0025 - mean_absolute_error: 0.0380 - learning_rate: 2.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - loss: 0.0022 - mean_absolute_error: 0.0362\n",
      "Epoch 48: loss improved from 0.00247 to 0.00229, saving model to checkpoints/weights_prev_multiple.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 204ms/step - loss: 0.0022 - mean_absolute_error: 0.0362 - learning_rate: 2.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - loss: 0.0024 - mean_absolute_error: 0.0380\n",
      "Epoch 49: loss did not improve from 0.00229\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 217ms/step - loss: 0.0024 - mean_absolute_error: 0.0380 - learning_rate: 2.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - loss: 0.0026 - mean_absolute_error: 0.0386\n",
      "Epoch 50: loss did not improve from 0.00229\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 265ms/step - loss: 0.0026 - mean_absolute_error: 0.0386 - learning_rate: 2.0000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - loss: 0.0026 - mean_absolute_error: 0.0380\n",
      "Epoch 51: loss did not improve from 0.00229\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 238ms/step - loss: 0.0026 - mean_absolute_error: 0.0379 - learning_rate: 2.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - loss: 0.0023 - mean_absolute_error: 0.0365\n",
      "Epoch 52: loss did not improve from 0.00229\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 206ms/step - loss: 0.0023 - mean_absolute_error: 0.0365 - learning_rate: 2.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - loss: 0.0025 - mean_absolute_error: 0.0375\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 53: loss did not improve from 0.00229\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 212ms/step - loss: 0.0025 - mean_absolute_error: 0.0375 - learning_rate: 2.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 0.0023 - mean_absolute_error: 0.0368\n",
      "Epoch 54: loss did not improve from 0.00229\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 197ms/step - loss: 0.0023 - mean_absolute_error: 0.0368 - learning_rate: 4.0000e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - loss: 0.0024 - mean_absolute_error: 0.0365\n",
      "Epoch 55: loss did not improve from 0.00229\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 199ms/step - loss: 0.0024 - mean_absolute_error: 0.0365 - learning_rate: 4.0000e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - loss: 0.0025 - mean_absolute_error: 0.0374\n",
      "Epoch 56: loss did not improve from 0.00229\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 202ms/step - loss: 0.0025 - mean_absolute_error: 0.0374 - learning_rate: 4.0000e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 0.0027 - mean_absolute_error: 0.0392\n",
      "Epoch 57: loss did not improve from 0.00229\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 198ms/step - loss: 0.0027 - mean_absolute_error: 0.0391 - learning_rate: 4.0000e-05\n",
      "Epoch 58/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 0.0025 - mean_absolute_error: 0.0376\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 58: loss did not improve from 0.00229\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 201ms/step - loss: 0.0025 - mean_absolute_error: 0.0376 - learning_rate: 4.0000e-05\n",
      "Epoch 58: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7dde29f7a0b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, batch_size=32, callbacks=[reduce_plateau, model_checkpoint, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_open = base_test_csv.iloc[:, 1:2].values\n",
    "x_high = base_test_csv.iloc[:, 2:3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_complete = pd.concat((base_train_csv['Open'], base_test_csv['Open']), axis=0)\n",
    "inputs = base_complete[len(base_complete) - len(base_test_csv) - antec:].values\n",
    "inputs = inputs.reshape(-1, 1)\n",
    "inputs = normalizer.transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = [inputs[i-antec:i, 0:6] for i in range(antec, inputs.shape[0])]\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[15.748248, 15.940084],\n",
       "       [15.883489, 16.076231],\n",
       "       [16.010841, 16.204365],\n",
       "       [16.144493, 16.339174],\n",
       "       [16.280962, 16.476954],\n",
       "       [16.409538, 16.606667],\n",
       "       [16.53242 , 16.730669],\n",
       "       [16.641865, 16.841005],\n",
       "       [16.726898, 16.926489],\n",
       "       [16.78925 , 16.98904 ],\n",
       "       [16.846546, 17.046835],\n",
       "       [16.909597, 17.110788],\n",
       "       [17.006304, 17.20936 ],\n",
       "       [17.163242, 17.369427],\n",
       "       [17.371777, 17.581593],\n",
       "       [17.598293, 17.811163],\n",
       "       [17.813232, 18.028103],\n",
       "       [17.994276, 18.210005],\n",
       "       [18.17363 , 18.390516],\n",
       "       [18.38086 , 18.599909],\n",
       "       [18.616158, 18.837986],\n",
       "       [18.861364, 19.08588 ]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevs = model.predict(x_test)\n",
    "prevs = normalizer.inverse_transform(prevs)\n",
    "prevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevs.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(prevs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test, color='red', label='Preço Real')\n",
    "plt.plot(prevs, color='blue', label='Previsões')\n",
    "plt.title('Previsão dos preços das ações')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
